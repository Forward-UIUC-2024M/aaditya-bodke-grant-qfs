{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIC INDEXING\n",
    "\n",
    "## This notebook defines a step-by-step procedure for semantic indexing data in ElasticSearch.\n",
    "\n",
    "First we import the required libraries and set the important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from elasticsearch import Elasticsearch, exceptions, NotFoundError, helpers\n",
    "from time import sleep\n",
    "import xmltodict\n",
    "from mappings import mappings\n",
    "from ingest_pipeline import get_ingest_pipeline\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from lxml import etree\n",
    "from utils import data_utils, index_utils, embed_utils\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "from dotenv import load_dotenv\n",
    "from tiktoken import get_encoding\n",
    "from itertools import islice\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get sensitive configuration from .env file or define as environment variables\n",
    "ELASTICSEARCH_URL = os.getenv('ELASTICSEARCH_URL')\n",
    "ELASTIC_USERNAME = os.getenv('ELASTIC_USERNAME')\n",
    "ELASTIC_PASSWORD = os.getenv('ELASTIC_PASSWORD')\n",
    "OPENAI_KEY = os.getenv('OPENAI_KEY')\n",
    "\n",
    "# Define other configuration variables\n",
    "INDEX_NAME = 'sem_index'\n",
    "\n",
    "\n",
    "OAIclient = OpenAI(api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Point variables to the XML and XSD files of the grants data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRANTS_FILE = '../data/grants.xml'\n",
    "GRANTS_SCHEMA = '../data/grants-20230530.xsd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the XML file using the schema defined in XSD file.\n",
    "\n",
    "Go through the errors, if any, to ensure they are not critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML is valid according to XSD\n"
     ]
    }
   ],
   "source": [
    "# Validate the XML file using schema\n",
    "data_utils.validate_xml_with_xsd(GRANTS_FILE, GRANTS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert XML data to dict format using function in data_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of grants = 10\n"
     ]
    }
   ],
   "source": [
    "dict_data = data_utils.parse_xml_to_dict(GRANTS_FILE)\n",
    "\n",
    "# Print data\n",
    "# with open('grants.json', 'w') as f: \n",
    "#     json.dump(dict_data, f)\n",
    "# with open('grants.json', 'r') as f: \n",
    "#     json_data = json.load(f)\n",
    "\n",
    "print(f\"Number of grants = {len(dict_data['grants_data']['grant'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data using function in data_utils. This function renames some fields and converts some to required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = data_utils.clean_dict_data(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['@id', 'url', 'amount_info', 'site_grant_type', 'modified_date', 'application_url', 'title', 'all_titles', 'submission_info', 'all_grant_source_urls', 'status', 'description',\n",
      "'eligibility', 'categories_display', 'limited_grant_info', 'user_categories', 'submit_date', 'is_limited', 'site_categories', 'cost_sharing', 'grant_source_url', 'deadlines', 'amounts', 'all_types',\n",
      "'all_applicant_types', 'locations', 'sponsors'])\n"
     ]
    }
   ],
   "source": [
    "print(fill(str(dict_data['grants_data']['grant'][0].keys()), width=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find token limits for longer fields in the documents. For simplicity, we will truncate the fields before creating embeddings for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_limit_index(tokenizer, text, max_tokens=8191):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # Return the full length of the text if it has fewer or equal tokens\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return len(text)  \n",
    "\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_text = tokenizer.decode(truncated_tokens)\n",
    "    \n",
    "    # Find the last character index of the truncated text in the original text\n",
    "    index = text.find(truncated_text) + len(truncated_text)\n",
    "    return index\n",
    "\n",
    "\n",
    "tokenizer = get_encoding(\"cl100k_base\")\n",
    "for data in dict_data['grants_data']['grant']:\n",
    "    data['description_truncate_length'] = find_token_limit_index(tokenizer, data['description'])\n",
    "    data['submission_info_truncate_length'] = find_token_limit_index(tokenizer, data['submission_info'])\n",
    "    data['eligibility_truncate_length'] = find_token_limit_index(tokenizer, data['eligibility'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to your ElasticSearch client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '601bb54c777c', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'FS5t3wmfS3y-5pkG0ZWv-A', 'version': {'number': '8.14.1', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '93a57a1a76f556d8aee6a90d1a95b06187501310', 'build_date': '2024-06-10T23:35:17.114581191Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connecting to ElasticSearch container\n",
    "ESclient = Elasticsearch(\n",
    "  ELASTICSEARCH_URL,\n",
    "  basic_auth = (ELASTIC_USERNAME, ELASTIC_PASSWORD),\n",
    "  request_timeout = 60\n",
    ")\n",
    "\n",
    "ESclient.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index with settings as defined in mappings.py. Alter the \"embeddings\" field to according to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'sem_index' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# ESclient.indices.delete(index = INDEX_NAME)\n",
    "index_utils.create_index(ESclient, INDEX_NAME, mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inference endpoint within the ElasticSearch for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: BadRequestError(400, 'resource_already_exists_exception', 'Inference model [openai-embeddings] already exists')\n"
     ]
    }
   ],
   "source": [
    "# Add OpenAI inference endpoint for embeddings\n",
    "INFERENCE_ID = \"openai-embeddings\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "try: \n",
    "    resp = ESclient.inference.put_model(\n",
    "        task_type = \"text_embedding\",\n",
    "        inference_id = INFERENCE_ID,\n",
    "        body = {\n",
    "            \"service\": \"openai\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": OPENAI_KEY,\n",
    "                \"model_id\": EMBEDDING_MODEL,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ingestion pipeline that adds embeddings to documents as they are indexed. It is defined in ingest_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ID = \"sem_embedding_pipeline\"\n",
    "pipeline_json = get_ingest_pipeline(INFERENCE_ID)\n",
    "\n",
    "# ESclient.ingest.delete_pipeline(id=pipeline_id)\n",
    "try:\n",
    "    resp = ESclient.ingest.put_pipeline(\n",
    "        id=PIPELINE_ID,\n",
    "        body = pipeline_json\n",
    "    )\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while (batch := tuple(islice(it, n))):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = index_utils.construct_indexing_actions(dict_data, INDEX_NAME, pipeline_id=PIPELINE_ID)\n",
    "batches = embed_utils.batched(body, 100)\n",
    "# print(f\"Total batches = {len(batches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 10 documents.\n",
      "Batch 0 done\n"
     ]
    }
   ],
   "source": [
    "for n,batch in enumerate(batches):\n",
    "    index_utils.bulk_index_documents(ESclient, batch, chunk_size=100)\n",
    "    print(f\"Batch {n} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_all': {'primaries': {'docs': {'count': 10, 'deleted': 0, 'total_size_in_bytes': 859139}, 'shard_stats': {'total_count': 1}, 'store': {'size_in_bytes': 859471, 'total_data_set_size_in_bytes': 859471, 'reserved_in_bytes': 0}, 'indexing': {'index_total': 10, 'index_time_in_millis': 72, 'index_current': 0, 'index_failed': 0, 'delete_total': 0, 'delete_time_in_millis': 0, 'delete_current': 0, 'noop_update_total': 0, 'is_throttled': False, 'throttle_time_in_millis': 0, 'write_load': 0.0007957248541119825}, 'get': {'total': 0, 'time_in_millis': 0, 'exists_total': 0, 'exists_time_in_millis': 0, 'missing_total': 0, 'missing_time_in_millis': 0, 'current': 0}, 'search': {'open_contexts': 0, 'query_total': 0, 'query_time_in_millis': 0, 'query_current': 0, 'fetch_total': 0, 'fetch_time_in_millis': 0, 'fetch_current': 0, 'scroll_total': 0, 'scroll_time_in_millis': 0, 'scroll_current': 0, 'suggest_total': 0, 'suggest_time_in_millis': 0, 'suggest_current': 0}, 'merges': {'current': 0, 'current_docs': 0, 'current_size_in_bytes': 0, 'total': 0, 'total_time_in_millis': 0, 'total_docs': 0, 'total_size_in_bytes': 0, 'total_stopped_time_in_millis': 0, 'total_throttled_time_in_millis': 0, 'total_auto_throttle_in_bytes': 20971520}, 'refresh': {'total': 5, 'total_time_in_millis': 1, 'external_total': 4, 'external_total_time_in_millis': 0, 'listeners': 0}, 'flush': {'total': 1, 'periodic': 1, 'total_time_in_millis': 70, 'total_time_excluding_waiting_on_lock_in_millis': 70}, 'warmer': {'current': 0, 'total': 2, 'total_time_in_millis': 0}, 'query_cache': {'memory_size_in_bytes': 0, 'total_count': 0, 'hit_count': 0, 'miss_count': 0, 'cache_size': 0, 'cache_count': 0, 'evictions': 0}, 'fielddata': {'memory_size_in_bytes': 0, 'evictions': 0, 'global_ordinals': {'build_time_in_millis': 0}}, 'completion': {'size_in_bytes': 0}, 'segments': {'count': 1, 'memory_in_bytes': 0, 'terms_memory_in_bytes': 0, 'stored_fields_memory_in_bytes': 0, 'term_vectors_memory_in_bytes': 0, 'norms_memory_in_bytes': 0, 'points_memory_in_bytes': 0, 'doc_values_memory_in_bytes': 0, 'index_writer_memory_in_bytes': 0, 'version_map_memory_in_bytes': 0, 'fixed_bit_set_memory_in_bytes': 0, 'max_unsafe_auto_id_timestamp': -1, 'file_sizes': {}}, 'translog': {'operations': 0, 'size_in_bytes': 55, 'uncommitted_operations': 0, 'uncommitted_size_in_bytes': 55, 'earliest_last_modified_age': 22227}, 'request_cache': {'memory_size_in_bytes': 0, 'evictions': 0, 'hit_count': 0, 'miss_count': 0}, 'recovery': {'current_as_source': 0, 'current_as_target': 0, 'throttle_time_in_millis': 0}, 'bulk': {'total_operations': 1, 'total_time_in_millis': 137, 'total_size_in_bytes': 815540, 'avg_time_in_millis': 13, 'avg_size_in_bytes': 81554}, 'dense_vector': {'value_count': 10}}, 'total': {'docs': {'count': 10, 'deleted': 0, 'total_size_in_bytes': 859139}, 'shard_stats': {'total_count': 1}, 'store': {'size_in_bytes': 859471, 'total_data_set_size_in_bytes': 859471, 'reserved_in_bytes': 0}, 'indexing': {'index_total': 10, 'index_time_in_millis': 72, 'index_current': 0, 'index_failed': 0, 'delete_total': 0, 'delete_time_in_millis': 0, 'delete_current': 0, 'noop_update_total': 0, 'is_throttled': False, 'throttle_time_in_millis': 0, 'write_load': 0.0007957248541119825}, 'get': {'total': 0, 'time_in_millis': 0, 'exists_total': 0, 'exists_time_in_millis': 0, 'missing_total': 0, 'missing_time_in_millis': 0, 'current': 0}, 'search': {'open_contexts': 0, 'query_total': 0, 'query_time_in_millis': 0, 'query_current': 0, 'fetch_total': 0, 'fetch_time_in_millis': 0, 'fetch_current': 0, 'scroll_total': 0, 'scroll_time_in_millis': 0, 'scroll_current': 0, 'suggest_total': 0, 'suggest_time_in_millis': 0, 'suggest_current': 0}, 'merges': {'current': 0, 'current_docs': 0, 'current_size_in_bytes': 0, 'total': 0, 'total_time_in_millis': 0, 'total_docs': 0, 'total_size_in_bytes': 0, 'total_stopped_time_in_millis': 0, 'total_throttled_time_in_millis': 0, 'total_auto_throttle_in_bytes': 20971520}, 'refresh': {'total': 5, 'total_time_in_millis': 1, 'external_total': 4, 'external_total_time_in_millis': 0, 'listeners': 0}, 'flush': {'total': 1, 'periodic': 1, 'total_time_in_millis': 70, 'total_time_excluding_waiting_on_lock_in_millis': 70}, 'warmer': {'current': 0, 'total': 2, 'total_time_in_millis': 0}, 'query_cache': {'memory_size_in_bytes': 0, 'total_count': 0, 'hit_count': 0, 'miss_count': 0, 'cache_size': 0, 'cache_count': 0, 'evictions': 0}, 'fielddata': {'memory_size_in_bytes': 0, 'evictions': 0, 'global_ordinals': {'build_time_in_millis': 0}}, 'completion': {'size_in_bytes': 0}, 'segments': {'count': 1, 'memory_in_bytes': 0, 'terms_memory_in_bytes': 0, 'stored_fields_memory_in_bytes': 0, 'term_vectors_memory_in_bytes': 0, 'norms_memory_in_bytes': 0, 'points_memory_in_bytes': 0, 'doc_values_memory_in_bytes': 0, 'index_writer_memory_in_bytes': 0, 'version_map_memory_in_bytes': 0, 'fixed_bit_set_memory_in_bytes': 0, 'max_unsafe_auto_id_timestamp': -1, 'file_sizes': {}}, 'translog': {'operations': 0, 'size_in_bytes': 55, 'uncommitted_operations': 0, 'uncommitted_size_in_bytes': 55, 'earliest_last_modified_age': 22227}, 'request_cache': {'memory_size_in_bytes': 0, 'evictions': 0, 'hit_count': 0, 'miss_count': 0}, 'recovery': {'current_as_source': 0, 'current_as_target': 0, 'throttle_time_in_millis': 0}, 'bulk': {'total_operations': 1, 'total_time_in_millis': 137, 'total_size_in_bytes': 815540, 'avg_time_in_millis': 13, 'avg_size_in_bytes': 81554}, 'dense_vector': {'value_count': 10}}}, 'indices': {'sem_index': {'uuid': 'rIH5cVQTQMKx7kcYmTCpDQ', 'health': 'yellow', 'status': 'open', 'primaries': {'docs': {'count': 10, 'deleted': 0, 'total_size_in_bytes': 859139}, 'shard_stats': {'total_count': 1}, 'store': {'size_in_bytes': 859471, 'total_data_set_size_in_bytes': 859471, 'reserved_in_bytes': 0}, 'indexing': {'index_total': 10, 'index_time_in_millis': 72, 'index_current': 0, 'index_failed': 0, 'delete_total': 0, 'delete_time_in_millis': 0, 'delete_current': 0, 'noop_update_total': 0, 'is_throttled': False, 'throttle_time_in_millis': 0, 'write_load': 0.0007957248541119825}, 'get': {'total': 0, 'time_in_millis': 0, 'exists_total': 0, 'exists_time_in_millis': 0, 'missing_total': 0, 'missing_time_in_millis': 0, 'current': 0}, 'search': {'open_contexts': 0, 'query_total': 0, 'query_time_in_millis': 0, 'query_current': 0, 'fetch_total': 0, 'fetch_time_in_millis': 0, 'fetch_current': 0, 'scroll_total': 0, 'scroll_time_in_millis': 0, 'scroll_current': 0, 'suggest_total': 0, 'suggest_time_in_millis': 0, 'suggest_current': 0}, 'merges': {'current': 0, 'current_docs': 0, 'current_size_in_bytes': 0, 'total': 0, 'total_time_in_millis': 0, 'total_docs': 0, 'total_size_in_bytes': 0, 'total_stopped_time_in_millis': 0, 'total_throttled_time_in_millis': 0, 'total_auto_throttle_in_bytes': 20971520}, 'refresh': {'total': 5, 'total_time_in_millis': 1, 'external_total': 4, 'external_total_time_in_millis': 0, 'listeners': 0}, 'flush': {'total': 1, 'periodic': 1, 'total_time_in_millis': 70, 'total_time_excluding_waiting_on_lock_in_millis': 70}, 'warmer': {'current': 0, 'total': 2, 'total_time_in_millis': 0}, 'query_cache': {'memory_size_in_bytes': 0, 'total_count': 0, 'hit_count': 0, 'miss_count': 0, 'cache_size': 0, 'cache_count': 0, 'evictions': 0}, 'fielddata': {'memory_size_in_bytes': 0, 'evictions': 0, 'global_ordinals': {'build_time_in_millis': 0}}, 'completion': {'size_in_bytes': 0}, 'segments': {'count': 1, 'memory_in_bytes': 0, 'terms_memory_in_bytes': 0, 'stored_fields_memory_in_bytes': 0, 'term_vectors_memory_in_bytes': 0, 'norms_memory_in_bytes': 0, 'points_memory_in_bytes': 0, 'doc_values_memory_in_bytes': 0, 'index_writer_memory_in_bytes': 0, 'version_map_memory_in_bytes': 0, 'fixed_bit_set_memory_in_bytes': 0, 'max_unsafe_auto_id_timestamp': -1, 'file_sizes': {}}, 'translog': {'operations': 0, 'size_in_bytes': 55, 'uncommitted_operations': 0, 'uncommitted_size_in_bytes': 55, 'earliest_last_modified_age': 22227}, 'request_cache': {'memory_size_in_bytes': 0, 'evictions': 0, 'hit_count': 0, 'miss_count': 0}, 'recovery': {'current_as_source': 0, 'current_as_target': 0, 'throttle_time_in_millis': 0}, 'bulk': {'total_operations': 1, 'total_time_in_millis': 137, 'total_size_in_bytes': 815540, 'avg_time_in_millis': 13, 'avg_size_in_bytes': 81554}, 'dense_vector': {'value_count': 10}}, 'total': {'docs': {'count': 10, 'deleted': 0, 'total_size_in_bytes': 859139}, 'shard_stats': {'total_count': 1}, 'store': {'size_in_bytes': 859471, 'total_data_set_size_in_bytes': 859471, 'reserved_in_bytes': 0}, 'indexing': {'index_total': 10, 'index_time_in_millis': 72, 'index_current': 0, 'index_failed': 0, 'delete_total': 0, 'delete_time_in_millis': 0, 'delete_current': 0, 'noop_update_total': 0, 'is_throttled': False, 'throttle_time_in_millis': 0, 'write_load': 0.0007957248541119825}, 'get': {'total': 0, 'time_in_millis': 0, 'exists_total': 0, 'exists_time_in_millis': 0, 'missing_total': 0, 'missing_time_in_millis': 0, 'current': 0}, 'search': {'open_contexts': 0, 'query_total': 0, 'query_time_in_millis': 0, 'query_current': 0, 'fetch_total': 0, 'fetch_time_in_millis': 0, 'fetch_current': 0, 'scroll_total': 0, 'scroll_time_in_millis': 0, 'scroll_current': 0, 'suggest_total': 0, 'suggest_time_in_millis': 0, 'suggest_current': 0}, 'merges': {'current': 0, 'current_docs': 0, 'current_size_in_bytes': 0, 'total': 0, 'total_time_in_millis': 0, 'total_docs': 0, 'total_size_in_bytes': 0, 'total_stopped_time_in_millis': 0, 'total_throttled_time_in_millis': 0, 'total_auto_throttle_in_bytes': 20971520}, 'refresh': {'total': 5, 'total_time_in_millis': 1, 'external_total': 4, 'external_total_time_in_millis': 0, 'listeners': 0}, 'flush': {'total': 1, 'periodic': 1, 'total_time_in_millis': 70, 'total_time_excluding_waiting_on_lock_in_millis': 70}, 'warmer': {'current': 0, 'total': 2, 'total_time_in_millis': 0}, 'query_cache': {'memory_size_in_bytes': 0, 'total_count': 0, 'hit_count': 0, 'miss_count': 0, 'cache_size': 0, 'cache_count': 0, 'evictions': 0}, 'fielddata': {'memory_size_in_bytes': 0, 'evictions': 0, 'global_ordinals': {'build_time_in_millis': 0}}, 'completion': {'size_in_bytes': 0}, 'segments': {'count': 1, 'memory_in_bytes': 0, 'terms_memory_in_bytes': 0, 'stored_fields_memory_in_bytes': 0, 'term_vectors_memory_in_bytes': 0, 'norms_memory_in_bytes': 0, 'points_memory_in_bytes': 0, 'doc_values_memory_in_bytes': 0, 'index_writer_memory_in_bytes': 0, 'version_map_memory_in_bytes': 0, 'fixed_bit_set_memory_in_bytes': 0, 'max_unsafe_auto_id_timestamp': -1, 'file_sizes': {}}, 'translog': {'operations': 0, 'size_in_bytes': 55, 'uncommitted_operations': 0, 'uncommitted_size_in_bytes': 55, 'earliest_last_modified_age': 22227}, 'request_cache': {'memory_size_in_bytes': 0, 'evictions': 0, 'hit_count': 0, 'miss_count': 0}, 'recovery': {'current_as_source': 0, 'current_as_target': 0, 'throttle_time_in_millis': 0}, 'bulk': {'total_operations': 1, 'total_time_in_millis': 137, 'total_size_in_bytes': 815540, 'avg_time_in_millis': 13, 'avg_size_in_bytes': 81554}, 'dense_vector': {'value_count': 10}}}}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESclient.indices.refresh(index=INDEX_NAME)\n",
    "ESclient.indices.stats(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all docs are indexed\n",
    "len(dict_data['grants_data']['grant']) - ESclient.indices.stats(index=INDEX_NAME)['_all']['primaries']['docs']['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# find ids of docs which are not indexed\n",
    "ids = [item['@id'] for item in dict_data['grants_data']['grant']]\n",
    "failed_ids = index_utils.get_missing_ids(ESclient, INDEX_NAME, ids)\n",
    "print(len(failed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = index_utils.construct_actions_from_ids(dict_data, failed_ids, INDEX_NAME, pipeline_id=PIPELINE_ID)\n",
    "batches = embed_utils.batched(body, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,batch in enumerate(batches):\n",
    "    index_utils.bulk_index_documents(ESclient, batch, chunk_size=100)\n",
    "    print(f\"Batch {n} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(index_utils.get_missing_ids(ESclient, INDEX_NAME, ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# find ids of docs that where creation of embeddings failed\n",
    "failed_ids = index_utils.get_failed_embedding_ids(ESclient, INDEX_NAME)\n",
    "print(len(failed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = index_utils.construct_actions_from_ids(dict_data, failed_ids, INDEX_NAME, pipeline_id=PIPELINE_ID)\n",
    "batches = embed_utils.batched(body, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,batch in enumerate(batches):\n",
    "    index_utils.bulk_index_documents(ESclient, batch, chunk_size=100)\n",
    "    print(f\"Batch {n} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(index_utils.get_failed_embedding_ids(ESclient, INDEX_NAME)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
